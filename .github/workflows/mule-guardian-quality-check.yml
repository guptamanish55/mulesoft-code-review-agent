name: Mule Guardian Code Quality Check (Reusable)

# THIS IS A REUSABLE WORKFLOW - Called from other repositories
on:
  workflow_call:
    inputs:
      # Input parameters that calling repositories can customize
      compliance_threshold:
        description: 'Minimum compliance percentage required'
        required: false
        type: number
        default: 75
      pmd_version:
        description: 'PMD version to use'
        required: false
        type: string
        default: '7.15.0'
      analysis_target:
        description: 'Directory to analyze (relative to repo root)'
        required: false
        type: string
        default: '.'
      project_name:
        description: 'Name of the project being analyzed (defaults to repository name)'
        required: false
        type: string
        default: ''  # Will be dynamically set to repository name if empty
      skip_quality_gate:
        description: 'Skip quality gate check (for debugging)'
        required: false
        type: boolean
        default: false
      require_pmd:
        description: 'Fail workflow if PMD analysis fails (prevents misleading high scores from alternative analysis)'
        required: false
        type: boolean
        default: true
      # NEW: Configurable Compliance Scoring Parameters
      config_file:
        description: 'Path to compliance configuration file (optional)'
        required: false
        type: string
        default: ''
      file_weight:
        description: 'File-based compliance weight (0-100, default: 70)'
        required: false
        type: number
        default: 70
      severity_weight:
        description: 'Severity-based compliance weight (0-100, default: 30)'
        required: false
        type: number
        default: 30
      high_weight:
        description: 'HIGH priority violation weight (default: 10)'
        required: false
        type: number
        default: 10
      medium_weight:
        description: 'MEDIUM priority violation weight (default: 5)'
        required: false
        type: number
        default: 5
      low_weight:
        description: 'LOW priority violation weight (default: 2)'
        required: false
        type: number
        default: 2
      info_weight:
        description: 'INFO priority violation weight (default: 1)'
        required: false
        type: number
        default: 1
    outputs:
      # Outputs that calling workflows can use
      compliance_score:
        description: 'Calculated compliance percentage'
        value: ${{ jobs.code-quality-check.outputs.compliance_score }}
      total_violations:
        description: 'Total number of violations found'
        value: ${{ jobs.code-quality-check.outputs.total_violations }}
      quality_gate_passed:
        description: 'Whether quality gate passed'
        value: ${{ jobs.code-quality-check.outputs.quality_gate_passed }}

jobs:
  code-quality-check:
    runs-on: ubuntu-latest
    
    # Set outputs for the job
    outputs:
      compliance_score: ${{ steps.mule-analysis.outputs.COMPLIANCE_SCORE }}
      total_violations: ${{ steps.mule-analysis.outputs.TOTAL_VIOLATIONS }}
      quality_gate_passed: ${{ steps.quality-gate.outputs.GATE_PASSED }}
    
    steps:
    # Step 1: Get the calling repository's code
    - name: Checkout Application Code
      uses: actions/checkout@v4
      
    # Step 2: Get Mule Guardian files from this common repository
    - name: Get Mule Guardian Core Files
      uses: actions/checkout@v4
      with:
        repository: ${{ github.repository_owner }}/mulesoft-code-review-agent  # ← Updated to match your actual repo
        path: mule-guardian
        ref: main
        
    # Step 3: Copy core files to working directory
    - name: Setup Mule Guardian Files
      run: |
        echo "📋 Setting up Mule Guardian files..."
        
        # Copy core files from the common repo checkout
        cp mule-guardian/mulesoft_ai_code_review_agent.py .
        cp mule-guardian/compliance_config.py .
        cp mule-guardian/comprehensive-mulesoft-ruleset-no-debug.xml .
        cp mule-guardian/requirements.txt .
        
        echo "✅ Core files copied from common repository"
        
        # Copy custom config file if specified
        if [ -n "${{ inputs.config_file }}" ] && [ -f "${{ inputs.config_file }}" ]; then
          echo "📝 Copying custom configuration file: ${{ inputs.config_file }}"
          cp "${{ inputs.config_file }}" compliance_config.json
        fi
        
        # Verify files exist
        for file in mulesoft_ai_code_review_agent.py compliance_config.py comprehensive-mulesoft-ruleset-no-debug.xml requirements.txt; do
          if [ ! -f "$file" ]; then
            echo "❌ Error: $file not found"
            exit 1
          fi
          echo "✅ Found: $file"
        done
      
    # Step 4: Setup Python
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    # Step 5: Setup Java (required for PMD)
    - name: Set up Java
      uses: actions/setup-java@v4
      with:
        distribution: 'temurin'
        java-version: '11'
        
    # Step 6: Install PMD with BULLETPROOF GUARANTEED WORKING SETUP
    - name: Install PMD (BULLETPROOF VERSION)
      run: |
        echo "🚀 BULLETPROOF PMD Installation - Guaranteed to work!"
        
        # Force install essential tools
        sudo apt-get update -qq
        sudo apt-get install -y wget unzip bc
        
        # Set up bulletproof Java environment FIRST
        echo "☕ Setting up bulletproof Java environment..."
        export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
        export PATH="$JAVA_HOME/bin:$PATH"
        echo "JAVA_HOME=$JAVA_HOME" >> $GITHUB_ENV
        echo "$JAVA_HOME/bin" >> $GITHUB_PATH
        
        echo "🔍 Java environment verification:"
        echo "  Java version: $(java -version 2>&1 | head -1)"
        echo "  JAVA_HOME: $JAVA_HOME"
        echo "  Java executable: $(which java)"
        
        # Download PMD using the specified version parameter with fallback URLs
        echo "📥 Downloading PMD ${{ inputs.pmd_version }}..."
        
        # Try different URL formats for PMD downloads (GitHub has different formats for different versions)
        if wget "https://github.com/pmd/pmd/releases/download/pmd_releases%2F${{ inputs.pmd_version }}/pmd-dist-${{ inputs.pmd_version }}-bin.zip" 2>/dev/null; then
          echo "✅ Downloaded PMD using pmd-dist format"
          unzip pmd-dist-${{ inputs.pmd_version }}-bin.zip
          PMD_DIR="pmd-bin-${{ inputs.pmd_version }}"
        elif wget "https://github.com/pmd/pmd/releases/download/pmd_releases%2F${{ inputs.pmd_version }}/pmd-bin-${{ inputs.pmd_version }}.zip" 2>/dev/null; then
          echo "✅ Downloaded PMD using pmd-bin format"
          unzip pmd-bin-${{ inputs.pmd_version }}.zip
          PMD_DIR="pmd-bin-${{ inputs.pmd_version }}"
        else
          echo "❌ Failed to download PMD ${{ inputs.pmd_version }}"
          echo "📋 Available PMD versions: https://github.com/pmd/pmd/releases"
          exit 1
        fi
        
        # Clean installation  
        sudo rm -rf /opt/pmd
        sudo mv "$PMD_DIR" /opt/pmd
        
        echo "✅ PMD ${{ inputs.pmd_version }} installed to /opt/pmd"
        
        # Verify installation files
        echo "🔍 PMD installation verification:"
        echo "  PMD directory: $(ls -la /opt/pmd/ | wc -l) files"
        echo "  PMD lib directory: $(ls -la /opt/pmd/lib/ | wc -l) JAR files"
        echo "  PMD bin directory: $(ls -la /opt/pmd/bin/ | wc -l) executables"
        
        # CRITICAL: Find the correct PMD main class for version ${{ inputs.pmd_version }}
        echo "🔍 Identifying correct PMD main class for version ${{ inputs.pmd_version }}..."
        
        # Test main classes in order of likelihood for PMD ${{ inputs.pmd_version }}
        MAIN_CLASSES=(
          "net.sourceforge.pmd.PMD"           # PMD 7.x (latest)
          "net.sourceforge.pmd.cli.PMD"       # PMD 7.x alternative
          "net.sourceforge.pmd.cli.PmdCli"    # PMD 6.x legacy
          "net.sourceforge.pmd.PMDCommandLine" # PMD 6.x alternative
        )
        
        WORKING_MAIN_CLASS=""
        for main_class in "${MAIN_CLASSES[@]}"; do
          echo "🧪 Testing main class: $main_class"
          if java -cp "/opt/pmd/lib/*" "$main_class" --help >/dev/null 2>&1; then
            WORKING_MAIN_CLASS="$main_class"
            echo "✅ FOUND WORKING MAIN CLASS: $WORKING_MAIN_CLASS"
            break
          else
            echo "❌ $main_class - not working"
            java -cp "/opt/pmd/lib/*" "$main_class" --help 2>&1 | head -2 || echo "  No output"
          fi
        done
        
        if [ -z "$WORKING_MAIN_CLASS" ]; then
          echo "🚨 CRITICAL ERROR: No working PMD main class found!"
          echo "🔍 Analyzing JAR contents:"
          for jar in /opt/pmd/lib/*.jar; do
            if [ -f "$jar" ]; then
              echo "📦 $(basename "$jar"): $(jar -tf "$jar" | grep -E 'PMD|pmd' | grep '\.class$' | head -2)"
            fi
          done
          exit 1
        fi
        
        echo "🎯 Using PMD main class: $WORKING_MAIN_CLASS"
        
        # Create BULLETPROOF PMD script using the correct main class
        echo "🔧 Creating bulletproof PMD execution script..."
        sudo mkdir -p /opt/pmd/bin
        
        # Write bulletproof script line by line to avoid YAML heredoc issues
        echo '#!/bin/bash' | sudo tee /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '# BULLETPROOF PMD Execution Script - Bypasses all shell script issues' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '# Generated automatically with correct main class' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '# Set up Java environment' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo 'if [ -z "$JAVA_HOME" ]; then' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '    export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo 'fi' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo 'export PATH="$JAVA_HOME/bin:$PATH"' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo '# Execute PMD directly with correct main class and classpath' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        echo 'exec "$JAVA_HOME/bin/java" -cp "/opt/pmd/lib/*" '$WORKING_MAIN_CLASS' "$@"' | sudo tee -a /opt/pmd/bin/pmd-bulletproof > /dev/null
        
        sudo chmod +x /opt/pmd/bin/pmd-bulletproof
        
        # Test the bulletproof script with PMD 6.x compatible commands
        echo "🧪 Testing bulletproof PMD script..."
        if /opt/pmd/bin/pmd-bulletproof --version >/dev/null 2>&1; then
          echo "✅ BULLETPROOF PMD script working perfectly!"
        elif /opt/pmd/bin/pmd-bulletproof -v >/dev/null 2>&1; then
          echo "✅ BULLETPROOF PMD script working (with -v)!"
        else
          echo "🚨 Bulletproof script failed - showing error:"
          /opt/pmd/bin/pmd-bulletproof --version 2>&1 | head -5
          echo "🧪 Trying alternative version check:"
          /opt/pmd/bin/pmd-bulletproof -v 2>&1 | head -5
          exit 1
        fi
        
        # Create all expected PMD paths using bulletproof script
        echo "🔗 Setting up all PMD paths with bulletproof script..."
        sudo mkdir -p /opt/homebrew/bin
        sudo ln -sf /opt/pmd/bin/pmd-bulletproof /opt/homebrew/bin/pmd
        sudo ln -sf /opt/pmd/bin/pmd-bulletproof /opt/pmd/bin/pmd
        sudo ln -sf /opt/pmd/bin/pmd-bulletproof /opt/pmd/bin/pmd-direct
        sudo ln -sf /opt/pmd/bin/pmd-bulletproof /opt/pmd/bin/pmd-safe
        sudo ln -sf /opt/pmd/bin/pmd-bulletproof /opt/pmd/bin/pmd-wrapper
        
        # Add to PATH
        echo "/opt/pmd/bin" >> $GITHUB_PATH
        echo "/opt/homebrew/bin" >> $GITHUB_PATH
        export PATH="/opt/pmd/bin:/opt/homebrew/bin:$PATH"
        
        # Final comprehensive testing
        echo "🧪 FINAL BULLETPROOF PMD TESTING..."
        
        # Test all paths with PMD 6.x compatible commands
        PMD_PATHS=("/opt/homebrew/bin/pmd" "/opt/pmd/bin/pmd" "/opt/pmd/bin/pmd-bulletproof")
        for pmd_path in "${PMD_PATHS[@]}"; do
          echo "🧪 Testing: $pmd_path"
          if "$pmd_path" --version >/dev/null 2>&1; then
            echo "✅ $pmd_path - WORKING (--version)"
          elif "$pmd_path" -v >/dev/null 2>&1; then
            echo "✅ $pmd_path - WORKING (-v)"
          else
            echo "❌ $pmd_path - FAILED"
            echo "🔍 Error output:"
            "$pmd_path" --version 2>&1 | head -3
          fi
        done
        
        echo "🎉 BULLETPROOF PMD INSTALLATION COMPLETED!"
        echo "📍 Main class: $WORKING_MAIN_CLASS"
        echo "📍 Bulletproof script: /opt/pmd/bin/pmd-bulletproof"
        echo "📍 All PMD paths use bulletproof direct Java execution"
        
    # Step 7: Install Python Dependencies
    - name: Install Python Dependencies
      run: |
        echo "📦 Installing Python dependencies..."
        
        # Upgrade pip to latest version to better handle dependency resolution
        python -m pip install --upgrade pip
        
        # Install dependencies with better error handling
        echo "🔄 Installing from requirements.txt..."
        if pip install -r requirements.txt; then
          echo "✅ Python dependencies installed successfully"
        else
          echo "⚠️ Initial installation failed, trying with dependency resolver..."
          # Try with backtracking resolver for better conflict resolution
          pip install -r requirements.txt --use-deprecated=backtrack-on-build-failures || {
            echo "❌ Dependency installation failed"
            echo "📋 Installed packages before failure:"
            pip list
            echo "🔍 Checking for specific conflicts..."
            pip check || true
            exit 1
          }
          echo "✅ Python dependencies installed with conflict resolution"
        fi
        
        echo "📋 Final installed packages:"
        pip list
        
    # Step 8: Run Mule Guardian Analysis
    - name: Run Mule Guardian Analysis
      id: mule-analysis
      env:
        # Configurable Compliance Scoring Environment Variables
        COMPLIANCE_FILE_WEIGHT: ${{ inputs.file_weight }}
        COMPLIANCE_SEVERITY_WEIGHT: ${{ inputs.severity_weight }}
        COMPLIANCE_HIGH_WEIGHT: ${{ inputs.high_weight }}
        COMPLIANCE_MEDIUM_WEIGHT: ${{ inputs.medium_weight }}
        COMPLIANCE_LOW_WEIGHT: ${{ inputs.low_weight }}
        COMPLIANCE_INFO_WEIGHT: ${{ inputs.info_weight }}
      run: |
        echo "🔍 Starting Mule Guardian analysis..."
        
        # Display configurable compliance settings
        echo "⚙️ Configurable Compliance Settings:"
        echo "   - File-based weight: ${{ inputs.file_weight }}%"
        echo "   - Severity-based weight: ${{ inputs.severity_weight }}%"
        echo "   - HIGH violation weight: ${{ inputs.high_weight }}"
        echo "   - MEDIUM violation weight: ${{ inputs.medium_weight }}"
        echo "   - LOW violation weight: ${{ inputs.low_weight }}"
        echo "   - INFO violation weight: ${{ inputs.info_weight }}"
        if [ -n "${{ inputs.config_file }}" ]; then
          echo "   - Custom config file: ${{ inputs.config_file }}"
        else
          echo "   - Using environment variable configuration"
        fi
        echo ""
        
        # Set dynamic project name if not provided
        if [ -z "${{ inputs.project_name }}" ] || [ "${{ inputs.project_name }}" = "" ]; then
          PROJECT_NAME="${{ github.event.repository.name }}"
          echo "📍 Project: $PROJECT_NAME (auto-detected from repository)"
        else
          PROJECT_NAME="${{ inputs.project_name }}"
          echo "📍 Project: $PROJECT_NAME (user-provided)"
        fi
        
        echo "📂 Analyzing: ${{ inputs.analysis_target }}"
        echo "🏢 Repository: ${{ github.repository }}"
        echo "🌿 Branch: ${{ github.ref_name }}"
        
        # Validate analysis target
        if [ ! -d "${{ inputs.analysis_target }}" ]; then
          echo "❌ Analysis target directory not found: ${{ inputs.analysis_target }}"
          exit 1
        fi
        
        # Show what we're analyzing
        echo "📁 Contents of analysis target:"
        ls -la "${{ inputs.analysis_target }}"
        
        # Find XML files to analyze
        XML_COUNT=$(find "${{ inputs.analysis_target }}" -name "*.xml" -type f | wc -l)
        echo "📊 Found $XML_COUNT XML files to analyze"
        
        if [ $XML_COUNT -eq 0 ]; then
          echo "⚠️ No XML files found in ${{ inputs.analysis_target }}"
          echo "This might not be a MuleSoft project or files are in a different location."
        fi
        
        # Clear Python cache to ensure fresh execution
        echo "🧹 Clearing Python cache to ensure latest code is used..."
        find . -name "*.pyc" -delete
        find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
        
        # Run the analysis and capture output
        echo "🔍 Starting Mule Guardian analysis with ENHANCED DIAGNOSTICS..."
        echo "🚀 EXPECTED: Should see 'PMD 6.x FORMAT' messages and NO 'check --file-list' errors"
        echo "🚀 EXPECTED: Should see 'PARSING PMD XML OUTPUT' and violation count logs"
        echo "🚀 If you see 'Violations found: 180' but 'Total violations extracted: 0', it's a parsing bug"
        python -B mulesoft_ai_code_review_agent.py "${{ inputs.analysis_target }}" comprehensive-mulesoft-ruleset-no-debug.xml -o quality-report.json -v 2>&1 | tee analysis_output.log
        
        # Check what type of analysis was actually used
        echo ""
        echo "🧪 ANALYSIS METHOD VERIFICATION:"
        if grep -q "SUCCESS: Using FULL PMD analysis" analysis_output.log 2>/dev/null; then
          echo "✅ CONFIRMED: Full PMD analysis with comprehensive ruleset was used"
          
          # Check if PMD actually found violations
          VIOLATIONS_FOUND=$(grep -o "Violations found: [0-9]*" analysis_output.log 2>/dev/null | grep -o "[0-9]*" || echo "unknown")
          FILES_ANALYZED=$(grep -o "XML files analyzed: [0-9]*" analysis_output.log 2>/dev/null | grep -o "[0-9]*" || echo "unknown")
          VIOLATIONS_PARSED=$(grep -o "Total violations extracted: [0-9]*" analysis_output.log 2>/dev/null | grep -o "[0-9]*" || echo "unknown")
          
          echo "📊 PMD Analysis Details:"
          echo "   - Files analyzed by PMD: $FILES_ANALYZED"
          echo "   - Violations found by PMD: $VIOLATIONS_FOUND"
          echo "   - Violations parsed from XML: $VIOLATIONS_PARSED"
          
          # Check for parsing discrepancy
          if [ "$VIOLATIONS_FOUND" != "unknown" ] && [ "$VIOLATIONS_PARSED" != "unknown" ] && [ "$VIOLATIONS_FOUND" != "$VIOLATIONS_PARSED" ]; then
            echo ""
            echo "🚨 CRITICAL: XML PARSING BUG DETECTED!"
            echo "🚨 PMD found $VIOLATIONS_FOUND violations but only $VIOLATIONS_PARSED were parsed!"
            echo "🚨 This explains the 100% compliance score discrepancy!"
            echo "🚨 Check logs for namespace/XML structure issues in parsing"
            echo ""
          fi
          
          if [ "$VIOLATIONS_FOUND" = "0" ]; then
            echo ""
            echo "🚨 CRITICAL WARNING: PMD found 0 violations!"
            echo "🚨 This results in 100% compliance which contradicts UI (28%)"
            echo "🚨 Possible causes:"
            echo "   - PMD is not analyzing the right files"
            echo "   - Ruleset is not effective for this project type"
            echo "   - PMD rules are not matching the code patterns"
            echo "🚨 The UI compliance score (28%) is likely more accurate"
            echo ""
            echo "🔍 Check the logs above for:"
            echo "   - 'PROJECT ANALYSIS' section showing files found"
            echo "   - 'PMD OUTPUT ANALYSIS' section showing what PMD processed"
            echo "   - 'Ruleset preview' to verify rules are loaded"
          elif [ "$VIOLATIONS_FOUND" != "unknown" ] && [ "$VIOLATIONS_FOUND" -gt "0" ]; then
            echo "✅ PMD found $VIOLATIONS_FOUND violations - this should provide accurate compliance score"
          fi
          
        elif grep -q "ALTERNATIVE analysis" analysis_output.log 2>/dev/null; then
          echo "🚨 WARNING: Alternative analysis was used instead of PMD!"
          echo "🚨 This results in HIGHER compliance scores than the UI"
          echo "🚨 The UI (28% compliance) is more accurate than this result"
          echo "🚨 PMD failed to run properly - check the logs above for Java classpath errors"
          
          # Check if we should fail when PMD doesn't work
          if [ "${{ inputs.require_pmd }}" = "true" ]; then
            echo ""
            echo "💥 FAILING WORKFLOW: require_pmd=true and PMD analysis failed"
            echo "💡 This prevents misleading high compliance scores"
            echo "💡 Fix PMD installation issues or set require_pmd=false to allow alternative analysis"
            exit 1
          fi
        else
          echo "❓ UNKNOWN: Could not determine analysis method used"
          echo "❓ Check the analysis_output.log for more details"
        fi
        
        # 🔍 DEBUG: Check Configuration Parameters (TEMPORARY DEBUGGING)
        echo "=================================="
        echo "🔍 DEBUGGING COMPLIANCE CALCULATION"
        echo "=================================="
        echo "Input Parameters Received:"
        echo "  file_weight: ${{ inputs.file_weight }}"
        echo "  severity_weight: ${{ inputs.severity_weight }}"
        echo "  high_weight: ${{ inputs.high_weight }}"
        echo "  medium_weight: ${{ inputs.medium_weight }}"
        echo "  low_weight: ${{ inputs.low_weight }}"
        echo "  info_weight: ${{ inputs.info_weight }}"
        echo ""
        echo "🔍 TESTING COMPLIANCE CONFIG:"
        python3 -c "
import sys
sys.path.append('.')
try:
    from compliance_config import ComplianceConfig
    print('✅ compliance_config imported successfully')
    
    # Test default config
    default_config = ComplianceConfig()
    print(f'✅ Default config: file={default_config.file_based_weight}%, severity={default_config.severity_based_weight}%')
    print(f'✅ Default priorities: {default_config.priority_weights}')
    
    # Test configured config with workflow inputs
    configured_config = ComplianceConfig(
        file_based_weight=${{ inputs.file_weight }},
        severity_based_weight=${{ inputs.severity_weight }},
        priority_weights={
            'HIGH': ${{ inputs.high_weight }},
            'MEDIUM': ${{ inputs.medium_weight }},
            'LOW': ${{ inputs.low_weight }},
            'INFO': ${{ inputs.info_weight }}
        }
    )
    print(f'✅ Configured config: file={configured_config.file_based_weight}%, severity={configured_config.severity_based_weight}%')
    print(f'✅ Configured priorities: {configured_config.priority_weights}')
    
except Exception as e:
    print(f'❌ ERROR with compliance_config: {e}')
    import traceback
    traceback.print_exc()
"
        echo "=================================="
        
        # 🔧 CRITICAL FIX: Force Unified Compliance Calculation
        if [ -f "quality-report.json" ]; then
          echo "✅ Analysis completed successfully"
          
          # Debug analysis data first
          echo "📊 DEBUGGING ANALYSIS RESULTS:"
          echo "================================"
          echo "📋 Raw JSON Extract:"
          echo "  Total Violations: $(grep -o '\"total_violations\":[[:space:]]*[0-9]*' quality-report.json | grep -o '[0-9]*')"
          echo "  Stored Compliance: $(grep -o '\"compliance_percentage\":[[:space:]]*[0-9.]*' quality-report.json | grep -o '[0-9.]*')"
          echo ""
          echo "📊 Priority Breakdown:"
          python3 -c "import json; data=json.load(open('quality-report.json')); priorities=data.get('violations_by_priority', {}); [print('  ' + str(p) + ': ' + str(c)) for p,c in priorities.items()]; print('  TOTAL: ' + str(sum(priorities.values())))"
          
          # Force unified compliance calculation (ignore stored values)
          echo ""
          echo "🔧 FORCING UNIFIED COMPLIANCE CALCULATION"
          # Create a temporary Python script for complex calculation using echo
          echo 'import json' > calc_compliance.py
          echo 'import sys' >> calc_compliance.py
          echo 'sys.path.append(".")' >> calc_compliance.py
          echo '' >> calc_compliance.py
          echo 'try:' >> calc_compliance.py
          echo '    from compliance_config import calculate_compliance_percentage, ComplianceConfig' >> calc_compliance.py
          echo '    ' >> calc_compliance.py
          echo '    with open("quality-report.json", "r") as f:' >> calc_compliance.py
          echo '        data = json.load(f)' >> calc_compliance.py
          echo '    ' >> calc_compliance.py
          echo '    class MockReport:' >> calc_compliance.py
          echo '        def __init__(self, data):' >> calc_compliance.py
          echo '            self.files_scanned = data.get("files_scanned", 0)' >> calc_compliance.py
          echo '            self.total_violations = data.get("total_violations", 0)' >> calc_compliance.py
          echo '            self.violations_by_priority = data.get("violations_by_priority", {})' >> calc_compliance.py
          echo '            self.violations = []' >> calc_compliance.py
          echo '            for v in data.get("violations", []):' >> calc_compliance.py
          echo '                violation = type("MockViolation", (), {})()' >> calc_compliance.py
          echo '                violation.file_path = v.get("file_path", "")' >> calc_compliance.py
          echo '                self.violations.append(violation)' >> calc_compliance.py
          echo '    ' >> calc_compliance.py
          echo '    report = MockReport(data)' >> calc_compliance.py
          echo '    # Use configurable parameters from workflow inputs' >> calc_compliance.py
          echo '    config = ComplianceConfig(' >> calc_compliance.py
          echo '        file_based_weight=${{ inputs.file_weight }},' >> calc_compliance.py
          echo '        severity_based_weight=${{ inputs.severity_weight }},' >> calc_compliance.py
          echo '        priority_weights={' >> calc_compliance.py
          echo '            "HIGH": ${{ inputs.high_weight }},' >> calc_compliance.py
          echo '            "MEDIUM": ${{ inputs.medium_weight }},' >> calc_compliance.py
          echo '            "LOW": ${{ inputs.low_weight }},' >> calc_compliance.py
          echo '            "INFO": ${{ inputs.info_weight }}' >> calc_compliance.py
          echo '        }' >> calc_compliance.py
          echo '    )' >> calc_compliance.py
          echo '    compliance = calculate_compliance_percentage(report, config)' >> calc_compliance.py
          echo '    print(f"{compliance:.2f}")' >> calc_compliance.py
          echo '    ' >> calc_compliance.py
          echo 'except ImportError as e:' >> calc_compliance.py
          echo '    print("ERROR: compliance_config not available")' >> calc_compliance.py
          echo '    sys.exit(1)' >> calc_compliance.py
          echo 'except Exception as e:' >> calc_compliance.py
          echo '    print(f"ERROR: {e}")' >> calc_compliance.py
          echo '    sys.exit(1)' >> calc_compliance.py
          
          UNIFIED_COMPLIANCE=$(python3 calc_compliance.py)
          rm -f calc_compliance.py
          
          if [ $? -eq 0 ] && [ ! -z "$UNIFIED_COMPLIANCE" ]; then
            echo "✅ UNIFIED CALCULATION SUCCESS: $UNIFIED_COMPLIANCE%"
            COMPLIANCE=$UNIFIED_COMPLIANCE
          else
            echo "❌ UNIFIED CALCULATION FAILED - WORKFLOW WILL FAIL"
            echo "❌ This prevents misleading compliance scores"
            exit 1
          fi
          
          # Extract total violations for validation
          TOTAL_VIOLATIONS=$(python3 -c "import json; print(json.load(open('quality-report.json')).get('total_violations', 0))")
          
          # Validate logical consistency
          echo ""
          echo "🔍 LOGICAL CONSISTENCY CHECK:"
          echo "  Compliance: $COMPLIANCE%"
          echo "  Violations: $TOTAL_VIOLATIONS"
          
          # Validate logical consistency with updated realistic thresholds
          violations_per_file=$(echo "scale=2; $TOTAL_VIOLATIONS / 54" | bc -l 2>/dev/null || echo "0")
          
          # More realistic consistency checks based on violations per file
          if [ "$TOTAL_VIOLATIONS" -gt 300 ] && [ "$(echo "$COMPLIANCE > 40" | bc -l 2>/dev/null || echo "0")" -eq 1 ]; then
            echo ""
            echo "🚨 LOGICAL INCONSISTENCY DETECTED!"
            echo "🚨 $TOTAL_VIOLATIONS violations (>300) cannot reasonably have $COMPLIANCE% compliance"
            echo "🚨 Expected compliance with $TOTAL_VIOLATIONS violations: <40%"
            echo "🚨 This indicates a serious calculation bug"
            echo ""
            exit 1
          fi
          
          # Check for extremely high violations with high compliance
          if [ "$TOTAL_VIOLATIONS" -gt 200 ] && [ "$(echo "$COMPLIANCE > 60" | bc -l 2>/dev/null || echo "0")" -eq 1 ]; then
            echo ""
            echo "🚨 LOGICAL INCONSISTENCY DETECTED!"
            echo "🚨 $TOTAL_VIOLATIONS violations (>200) cannot have $COMPLIANCE% compliance"
            echo "🚨 Expected compliance with $TOTAL_VIOLATIONS violations: <60%"
            echo "🚨 This indicates a calculation error"
            echo ""
            exit 1
          fi
          
          # Warning for high violations
          if [ "$TOTAL_VIOLATIONS" -gt 100 ] && [ "$(echo "$COMPLIANCE > 70" | bc -l 2>/dev/null || echo "0")" -eq 1 ]; then
            echo "⚠️  WARNING: High violations ($TOTAL_VIOLATIONS) with high compliance ($COMPLIANCE%)"
            echo "⚠️  Violations per file: $violations_per_file - Please verify this is reasonable"
          fi
          
          echo "✅ Logical consistency check passed"
          
          echo "COMPLIANCE_SCORE=$COMPLIANCE" >> $GITHUB_OUTPUT
          echo "TOTAL_VIOLATIONS=$TOTAL_VIOLATIONS" >> $GITHUB_OUTPUT
          
          echo ""
          echo "🛡️ VERIFIED COMPLIANCE RESULTS:"
          echo "================================"
          echo "✅ Compliance Score: $COMPLIANCE% (UNIFIED CALCULATION)"
          echo "✅ Total Violations: $TOTAL_VIOLATIONS"
          echo "✅ Calculation Method: Configurable Unified"
          echo "✅ Logical Consistency: VALIDATED"
          echo ""
          echo "🔧 Configuration Used:"
          echo "  - File-based weight: ${{ inputs.file_weight }}%"
          echo "  - Severity-based weight: ${{ inputs.severity_weight }}%"
          echo "  - Priority weights: HIGH=${{ inputs.high_weight }}, MEDIUM=${{ inputs.medium_weight }}, LOW=${{ inputs.low_weight }}, INFO=${{ inputs.info_weight }}"
          
        else
          echo "❌ Analysis failed - no report generated"
          exit 1
        fi
        
    # Step 9: Quality Gate Check
    - name: Quality Gate Check
      id: quality-gate
      run: |
        SCORE=${{ steps.mule-analysis.outputs.COMPLIANCE_SCORE }}
        THRESHOLD=${{ inputs.compliance_threshold }}
        SKIP_GATE=${{ inputs.skip_quality_gate }}
        
        echo "📊 Quality Gate Check:"
        echo "   - Compliance Score: $SCORE%"
        echo "   - Required Threshold: $THRESHOLD%"
        echo "   - Skip Quality Gate: $SKIP_GATE"
        
        # Check for misleading compliance scores
        if grep -q "ALTERNATIVE analysis" analysis_output.log 2>/dev/null; then
          echo ""
          echo "🚨 CRITICAL WARNING: Alternative analysis was used!"
          echo "🚨 This score ($SCORE%) may be MISLEADINGLY HIGH"
          echo "🚨 The UI compliance score (28%) is likely more accurate"
          echo "🚨 PMD analysis failed - please fix PMD installation issues"
          echo ""
        elif grep -q "PMD found 0 violations" analysis_output.log 2>/dev/null; then
          echo ""
          echo "🚨 CRITICAL WARNING: PMD found 0 violations resulting in misleading 100% compliance!"
          echo "🚨 This contradicts the UI which shows 28% compliance"
          echo "🚨 PMD may not be analyzing the right files or using effective rules"
          echo "🚨 Check the detailed logs above for diagnosis"
          echo ""
          
          # Option to fail the workflow for misleading PMD results
          if [ "${{ inputs.require_pmd }}" = "true" ]; then
            echo "💥 FAILING WORKFLOW: PMD found 0 violations which is inconsistent with expected results"
            echo "💡 This prevents misleading high compliance scores"
            exit 1
          fi
        fi
        
        if [ "$SKIP_GATE" = "true" ]; then
          echo "⚠️ Quality gate check skipped"
          echo "GATE_PASSED=true" >> $GITHUB_OUTPUT
        elif (( $(echo "$SCORE >= $THRESHOLD" | bc -l) )); then
          echo "✅ QUALITY GATE PASSED"
          echo "GATE_PASSED=true" >> $GITHUB_OUTPUT
        else
          echo "❌ QUALITY GATE FAILED"
          echo "Project compliance ($SCORE%) is below required threshold ($THRESHOLD%)"
          echo "GATE_PASSED=false" >> $GITHUB_OUTPUT
          
          if [ "$SKIP_GATE" != "true" ]; then
            exit 1
          fi
        fi
        
    # Step 10: Generate Reports
    - name: Generate Reports
      if: always()
      run: |
        echo "📄 Generating HTML report..."
        
        # Extract values for HTML report
        COMPLIANCE_SCORE="${{ steps.mule-analysis.outputs.COMPLIANCE_SCORE }}"
        TOTAL_VIOLATIONS="${{ steps.mule-analysis.outputs.TOTAL_VIOLATIONS }}"
        PROJECT_NAME="${{ inputs.project_name }}"
        THRESHOLD="${{ inputs.compliance_threshold }}"
        
        # Generate HTML report using echo statements to avoid YAML conflicts
        echo "<!DOCTYPE html>" > quality-report.html
        echo "<html><head><title>Mule Guardian Quality Report - $PROJECT_NAME</title>" >> quality-report.html
        echo "<style>body{font-family:Arial;margin:40px;background:#f5f7fa}.header{background:#667eea;color:white;padding:20px;border-radius:8px;margin-bottom:20px}.metric{display:inline-block;background:white;margin:10px;padding:20px;border-radius:8px;min-width:120px;text-align:center;box-shadow:0 2px 4px rgba(0,0,0,0.1)}.metric h2{margin:0;font-size:2em}.metric p{margin:10px 0 0 0;color:#666}.passed{border-left:4px solid #28a745}.failed{border-left:4px solid #dc3545}.summary{background:white;padding:20px;border-radius:8px;margin:20px 0}</style>" >> quality-report.html
        echo "</head><body>" >> quality-report.html
        echo "<div class='header'><h1>🛡️ Mule Guardian Quality Report</h1><p>$PROJECT_NAME - Code Quality Analysis Results</p></div>" >> quality-report.html
        
        # Determine status class
        if [ $COMPLIANCE_SCORE -ge $THRESHOLD ]; then
          STATUS_CLASS="passed"
          STATUS_TEXT="✅ PASSED"
        else
          STATUS_CLASS="failed" 
          STATUS_TEXT="❌ FAILED"
        fi
        
        echo "<div class='metric $STATUS_CLASS'><h2>${COMPLIANCE_SCORE}%</h2><p>Compliance Score</p><small>Threshold: ${THRESHOLD}%</small></div>" >> quality-report.html
        echo "<div class='metric'><h2>$TOTAL_VIOLATIONS</h2><p>Total Violations</p></div>" >> quality-report.html
        echo "<div class='summary'>" >> quality-report.html
        echo "<h3>Analysis Summary</h3>" >> quality-report.html
        echo "<p><strong>Project:</strong> $PROJECT_NAME</p>" >> quality-report.html
        echo "<p><strong>Analysis Target:</strong> ${{ inputs.analysis_target }}</p>" >> quality-report.html
        echo "<p><strong>Quality Gate:</strong> $STATUS_TEXT</p>" >> quality-report.html
        echo "<p><strong>Timestamp:</strong> $(date)</p>" >> quality-report.html
        echo "</div>" >> quality-report.html
        echo "<p><em>Download the JSON report for detailed violation information.</em></p>" >> quality-report.html
        echo "</body></html>" >> quality-report.html
        
        echo "✅ HTML report generated successfully"
        
    # Step 11: Upload Reports as Artifacts
    - name: Upload Quality Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: mule-guardian-quality-report-${{ inputs.project_name }}
        path: |
          quality-report.json
          quality-report.html
        retention-days: 30
        
    # Step 12: Summary
    - name: Analysis Summary
      if: always()
      run: |
        echo "🎉 MULE GUARDIAN ANALYSIS COMPLETED!"
        echo ""
        echo "📊 Results for ${{ inputs.project_name }}:"
        echo "   - Compliance Score: ${{ steps.mule-analysis.outputs.COMPLIANCE_SCORE }}%"
        echo "   - Total Violations: ${{ steps.mule-analysis.outputs.TOTAL_VIOLATIONS }}"
        echo "   - Quality Gate: $([ "${{ steps.quality-gate.outputs.GATE_PASSED }}" = "true" ] && echo "PASSED" || echo "FAILED")"
        echo ""
        echo "📁 Reports uploaded as artifact: mule-guardian-quality-report-${{ inputs.project_name }}"
        echo "🔍 Download from Actions tab for detailed analysis"
